# CustomNeuralNetwork

A C++ neural network framework built from scratch using the LinearAlgebra library.

## ğŸ“ Structure

```
CustomNeuralNetwork/
â”œâ”€â”€ README.md
â”œâ”€â”€ include/
â”‚   â””â”€â”€ CustomNeuralNetwork/
â”‚       â”œâ”€â”€ NN.h
â”‚       â”œâ”€â”€ ActivationFunctions/
â”‚       â”‚   â”œâ”€â”€ ActivationFunctions.h
â”‚       â”‚   â”œâ”€â”€ BaseActivationFunction.h
â”‚       â”‚   â”œâ”€â”€ ReLUActivationFunction.h
â”‚       â”‚   â”œâ”€â”€ SigmoidActivationFunction.h
â”‚       â”‚   â””â”€â”€ TanhActivationFunction.h
â”‚       â”œâ”€â”€ LossFunctions/
â”‚       â”‚   â”œâ”€â”€ LossFunctions.h
â”‚       â”‚   â”œâ”€â”€ BaseLossFunction.h
â”‚       â”‚   â””â”€â”€ MeanSquaredErrorLossFunction.h
â”‚       â””â”€â”€ Optimizers/
â”‚           â”œâ”€â”€ Optimizers.h
â”‚           â”œâ”€â”€ BaseOptimizer.h
â”‚           â””â”€â”€ StochasticGDOptimizer.h
â””â”€â”€ src/
    â”œâ”€â”€ NN.cpp
    â”œâ”€â”€ ActivationFunctions/
    â”‚   â”œâ”€â”€ BaseActivationFunction.cpp
    â”‚   â”œâ”€â”€ ReLUActivationFunction.cpp
    â”‚   â”œâ”€â”€ SigmoidActivationFunction.cpp
    â”‚   â””â”€â”€ TanhActivationFunction.cpp
    â”œâ”€â”€ LossFunctions/
    â”‚   â”œâ”€â”€ BaseLossFunction.cpp
    â”‚   â””â”€â”€ MeanSquaredErrorLossFunction.cpp
    â””â”€â”€ Optimizers/
        â”œâ”€â”€ BaseOptimizer.cpp
        â””â”€â”€ StochasticGDOptimizer.cpp
```

## ğŸš€ Quick Start

### Create a Simple Neural Network

```cpp
#include "LinearAlgebra/include/AlgLin.h"
#include "MachineLearning/CustomNeuralNetwork/include/NN.h"

// Training data
Matrix x_train({{0, 0}, {1, 0}, {0, 1}, {1, 1}});
Matrix y_train({{0}, {0}, {0}, {1}});

// Create network (input size = 2)
NN network(2);
network.initialize();

// Configure network
network.setActivationFunction(std::make_unique<SigmoidActivationFunction>());
network.setLossFunction(std::make_unique<MeanSquaredErrorLossFunction>());

// Train
network.fit(x_train, y_train, 1000, 0.1);

// Predict
float prediction = network.predict({1.0f, 1.0f});
```

## âœ¨ Features

### Activation Functions

| Function | Equation | Use Case |
|----------|----------|----------|
| **ReLU** | max(0, x) | Hidden layers |
| **Sigmoid** | 1/(1+e^-x) | Binary classification |
| **Tanh** | (e^x - e^-x)/(e^x + e^-x) | Hidden layers, [-1, 1] output |

### Loss Functions

| Function | Formula | Use Case |
|----------|---------|----------|
| **MSE** | 1/n Î£(Å· - y)Â² | Regression, classification |

### Optimizers

| Optimizer | Update Rule | Learning Rate |
|-----------|------------|-----------------|
| **SGD** | w = w - lr Ã— âˆ‡L | Configurable |

## ğŸ“– API Reference

### NN Class

```cpp
// Constructor
NN(int inputSize);

// Configuration
void setActivationFunction(std::unique_ptr<BaseActivationFunction> func);
void setLossFunction(std::unique_ptr<BaseLossFunction> func);

// Training & Prediction
void initialize();
void fit(const Matrix& x_train, const Matrix& y_train, 
         int epochs = 100, float lr = 1e-3);
float predict(const std::initializer_list<float>& x);

// Getters
float getOutput() const;
const std::vector<float>& getLossHistory() const;
```

### BaseActivationFunction

```cpp
class BaseActivationFunction {
    virtual float activate(float x) const = 0;
    virtual float derivative(float x) const = 0;
};
```

### BaseLossFunction

```cpp
class BaseLossFunction {
    virtual float compute(const Matrix& predictions, 
                         const Matrix& targets) const = 0;
    virtual Matrix derivative(const Matrix& predictions, 
                             const Matrix& targets) const = 0;
};
```

## ğŸ”§ Compilation

Automatically compiled with main build script:

```powershell
./build.ps1
```

## ğŸ“Š Example: XOR Problem

```cpp
#include "LinearAlgebra/include/AlgLin.h"
#include "MachineLearning/CustomNeuralNetwork/include/NN.h"
#include <iostream>

int main() {
    // XOR training data
    Matrix x_train({
        {0, 0},
        {0, 1},
        {1, 0},
        {1, 1}
    });
    
    Matrix y_train({
        {0},
        {1},
        {1},
        {0}
    });

    // Create and configure network
    NN nn(2);
    nn.initialize();
    nn.setActivationFunction(std::make_unique<SigmoidActivationFunction>());
    nn.setLossFunction(std::make_unique<MeanSquaredErrorLossFunction>());

    // Train
    nn.fit(x_train, y_train, 5000, 0.5);

    // Test predictions
    std::cout << "Predictions:" << std::endl;
    std::cout << "0 XOR 0 = " << nn.predict({0, 0}) << std::endl;
    std::cout << "0 XOR 1 = " << nn.predict({0, 1}) << std::endl;
    std::cout << "1 XOR 0 = " << nn.predict({1, 0}) << std::endl;
    std::cout << "1 XOR 1 = " << nn.predict({1, 1}) << std::endl;

    // Loss history
    const auto& loss_hist = nn.getLossHistory();
    std::cout << "\nFinal loss: " << loss_hist.back() << std::endl;

    return 0;
}
```

## ğŸ¯ Current Limitations

- âš ï¸ Single neuron architecture (multi-layer in development)
- âš ï¸ No batch normalization
- âš ï¸ No regularization (L1/L2)
- âš ï¸ Limited optimizer support

## ğŸ”® Roadmap

- [ ] Multi-layer architecture
- [ ] Batch normalization
- [ ] Dropout regularization
- [ ] More optimizers (Adam, RMSprop)
- [ ] Convolutional layers
- [ ] Recurrent layers
- [ ] GPU acceleration

## ğŸ“„ License

MIT License