# C++ Projects Repository

A collection of C++ projects focusing on **Linear Algebra** and **Machine Learning** from scratch.

## ğŸ“ Project Structure

```
cpp-projects/
â”œâ”€â”€ README.md
â”œâ”€â”€ build.ps1                          (Build script)
â”œâ”€â”€ main.cpp                           (Entry point)
â”œâ”€â”€ LinearAlgebra/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â””â”€â”€ LinearAlgebra/
â”‚   â”‚       â”œâ”€â”€ LinAlg.h               (PUBLIC - main entry point)
â”‚   â”‚       â”œâ”€â”€ Matrix.h               (PUBLIC - Matrix API)
â”‚   â”‚       â”œâ”€â”€ Vector.h               (PUBLIC - Vector API)
â”‚   â”‚       â”œâ”€â”€ Shape.h                (PUBLIC - Shape class)
â”‚   â”‚       â””â”€â”€ Functions.h            (PUBLIC - utility functions)
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ Shape.cpp
â”‚
â”œâ”€â”€ MachineLearning/
â”‚    â””â”€â”€ CustomNeuralNetwork/
â”‚        â”œâ”€â”€ README.md
â”‚        â”œâ”€â”€ include/
â”‚        â”‚   â””â”€â”€ CustomNeuralNetwork/
â”‚        â”‚       â”œâ”€â”€ NN.h                       (Main NN class)
â”‚        â”‚       â”œâ”€â”€ DenseLayer.h
â”‚        â”‚       â”œâ”€â”€ ActivationFunctions/
â”‚        â”‚       â”‚   â”œâ”€â”€ ActivationFunctions.h
â”‚        â”‚       â”‚   â”œâ”€â”€ BaseActivationFunction.h
â”‚        â”‚       â”‚   â”œâ”€â”€ ReLUActivationFunction.h
â”‚        â”‚       â”‚   â”œâ”€â”€ SigmoidActivationFunction.h
â”‚        â”‚       â”‚   â””â”€â”€ TanhActivationFunction.h
â”‚        â”‚       â”œâ”€â”€ InitializationFunctions/
â”‚        â”‚       â”‚   â”œâ”€â”€ InitializationFunctions.h
â”‚        â”‚       â”‚   â”œâ”€â”€ BaseInitializationFunction.h
â”‚        â”‚       â”‚   â”œâ”€â”€ HeInitializationFunction.h
â”‚        â”‚       â”‚   â””â”€â”€ XavierInitializationFunction.h
â”‚        â”‚       â”œâ”€â”€ LossFunctions/
â”‚        â”‚       â”‚   â”œâ”€â”€ LossFunctions.h
â”‚        â”‚       â”‚   â”œâ”€â”€ BaseLossFunction.h
â”‚        â”‚       â”‚   â””â”€â”€ MeanSquaredErrorLossFunction.h
â”‚        â”‚       â””â”€â”€ Optimizers/
â”‚        â”‚           â”œâ”€â”€ Optimizers.h
â”‚        â”‚           â”œâ”€â”€ BaseOptimizer.h
â”‚        â”‚           â”œâ”€â”€ AdaptativeMomentOptimizer.cpp (NOT IMPLEMENTED YET)
â”‚        â”‚           â””â”€â”€ StochasticGDOptimizer.h
â”‚        â””â”€â”€ src/
â”‚            â”œâ”€â”€ NN.cpp
â”‚            â”œâ”€â”€ DenseLayer.cpp
â”‚            â”œâ”€â”€ ActivationFunctions/
â”‚            â”‚   â”œâ”€â”€ BaseActivationFunction.cpp
â”‚            â”‚   â”œâ”€â”€ ReLUActivationFunction.cpp
â”‚            â”‚   â”œâ”€â”€ SigmoidActivationFunction.cpp
â”‚            â”‚   â””â”€â”€ TanhActivationFunction.cpp
â”‚            â”‚       â”œâ”€â”€ InitializationFunctions/
â”‚            â”‚       â”‚   â”œâ”€â”€ BaseInitializationFunction.cpp
â”‚            â”‚       â”‚   â”œâ”€â”€ HeInitializationFunction.cpp
â”‚            â”‚       â”‚   â””â”€â”€ XavierInitializationFunction.cpp
â”‚            â”œâ”€â”€ LossFunctions/
â”‚            â”‚   â”œâ”€â”€ BaseLossFunction.cpp
â”‚            â”‚   â””â”€â”€ MeanSquaredErrorLossFunction.cpp
â”‚            â””â”€â”€ Optimizers/
â”‚                â”œâ”€â”€ BaseOptimizer.cpp
â”‚                â”œâ”€â”€ AdaptativeMomentOptimizer.cpp (NOT IMPLEMENTED YET)
â”‚                â””â”€â”€ StochasticGDOptimizer.cpp
â””â”€â”€ Utils/
    â”œâ”€â”€ include/
    â”‚   â”œâ”€â”€ utils.h
    â”‚   â”œâ”€â”€ Utils/       
    â”‚   â”‚   â”œâ”€â”€ benchmark.h
    â”‚   â”‚   â””â”€â”€ print.h
    â””â”€â”€ src/
        â””â”€â”€ benchmark.cpp
```

## ğŸš€ Quick Start

### Prerequisites
- **Compiler**: Clang++ (MSYS2 clang64)
- **C++ Standard**: C++20
- **OS**: Windows (PowerShell)

### Build & Run

```powershell
# Navigate to workspace root
cd cpp-projects

# Build the project
./build.ps1

# Run the executable
./main.exe
```

### Alternative: Direct Compilation

```powershell
D:/msys64/clang64/bin/clang++.exe -std=c++20 -O3 `
  -I./LinearAlgebra/include `
  -I./MachineLearning/CustomNeuralNetwork/include `
  -I./Utils/include `
  (Get-ChildItem -Path './LinearAlgebra/src', './MachineLearning/CustomNeuralNetwork/src', './Utils/src', -Filter '*.cpp' -Recurse | ForEach-Object { $_.FullName }) `
  main.cpp -o neural-network.exe -Wall -Wextra
```

## ğŸ“š Projects

### 1. LinearAlgebra Library
A lightweight linear algebra library with Matrix and Vector support.

**Features:**
- âœ… Matrix operations (addition, multiplication, transpose)
- âœ… Vector operations
- âœ… Shape validation
- âœ… Template-based (float/double support)

**Location:** `LinearAlgebra/`

### 2. CustomNeuralNetwork
A neural network framework built from scratch using the LinearAlgebra library.

**Features:**
- âœ… Single neuron architecture (expandable)
- âœ… Multiple activation functions (ReLU, Sigmoid, Tanh)
- âœ… Loss functions (Mean Squared Error)
- âœ… Optimizers (Stochastic Gradient Descent)
- âœ… Forward & backward propagation
- âœ… Training with fit() method

**Location:** `MachineLearning/CustomNeuralNetwork/`

## ğŸ› ï¸ Build System

Currently using **direct clang++ compilation** via PowerShell script (`build.ps1`) for faster compile times.

**Future:** May migrate to CMake for larger projects.

## ğŸ“– Usage Examples

### LinearAlgebra

```cpp
#include <LinearAlgebra/LinAlg.h>

// Create matrices
Matrix m1({{1, 2}, {3, 4}});
Matrix m2({{5, 6}, {7, 8}});

// Operations
Matrix result = m1 + m2;
Matrix product = m1 * m2;
result.print();
```

### CustomNeuralNetwork

```cpp
#include <LinearAlgebra/LinAlg.h>
#include <CustomNeuralNetwork/NN.h>

// Training data (XOR problem)
Matrix x_train({{0, 0}, {1, 0}, {0, 1}, {1, 1}});
Matrix y_train({{0}, {1}, {1}, {0}});

// Create and train neural network
int input_dim = 2;
int n_layers = 1;
int layers_dim = 2;
int output_dim = 1;
float learning_rate = 1e-2;
int epochs = 1e6;
NN network(input_dim, n_layers, layers_dim, output_dim);
network.setInitializationFunction(RANDOM);
network.setActivationFunction(ReLU);
network.setLossFunction(MSE);
network.setOptimizer(SGD, learning_rate);
network.initialize();
network.fit(x_train, y_train, epochs);

// Predict
float prediction = network.predict({1.0f, 1.0f});
std::cout << "Prediction: " << prediction << std::endl;
```

## ğŸ”§ Development Notes

### Code Organization
- **Public headers** in `include/` - user-facing API
- **Private headers** in `src/` - internal implementation details
- **Template implementations** in `.h` and `.tpp` files

### Compilation Strategy
1. Forward declarations in headers reduce compile time
2. Full includes only in `.cpp` files
3. Templates instantiated explicitly where needed

### Performance Optimization
- Compiler flags: `-O3` (aggressive optimization)
- Link-time optimization ready
- SIMD-ready matrix operations (future)

## ğŸ¯ TODO / Roadmap

- [ ] Add more activation functions (LeakyReLU, ELU)
- [ ] Implement more optimizers (Adam, RMSprop)
- [ ] Add convolutional layers
- [ ] Performance benchmarks
- [ ] Unit tests framework

## ğŸ“ License

MIT License - Feel free to use for learning purposes.

## ğŸ‘¤ Author

Created by Thiago Souza - July 2024
Last Updated: 2026-01-31